{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_seq_items = 10000\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from selenium import webdriver\n",
    "import html5lib\n",
    "import lxml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369545b",
   "metadata": {},
   "source": [
    "### Campany Scrape\n",
    "\n",
    "Company Scrape The first step is to pull companies that have a top work-life balance for a role you are interested in. For example, pull companies with a work-life balance >= 3.5 stars for Analyst roles. This scrape will pull a max of ~9,900 companies as Glassdoor has a bug that doesn't show more companies after page 1,000 on the company explore section of the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ab0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create temp df\n",
    "df = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "length = 1\n",
    "i = 1\n",
    "#while statement will go to 1000 as Glassdoor doesn't share companies\n",
    "#after page 1000\n",
    "while (i <= 1000):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    \n",
    "    #scrape companies with DS jobs with WLB > 4.5 stars\n",
    "    #url = \"https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=4.5&page=\" + str(i) + \"&occ=Data%20Scientist&filterType=RATING_WORK_LIFE_BALANCE\"\n",
    "    \n",
    "    #scrape companies with Analyst jobs with WLB > 3.5 stars\n",
    "    url = \"https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=3.5&page=\" + str(i) + \"&occ=Analyst&filterType=RATING_WORK_LIFE_BALANCE\"\n",
    "    \n",
    "    #scrape companies with overall WLB > 4.5 stars\n",
    "    #url = \"https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=4.5&page=\" + str(i) + \"&filterType=RATING_WORK_LIFE_BALANCE\"\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    #table = soup.find_all('div', {'class':'col-md-8'})\n",
    "    #table = soup.find_all('div', {'class':'mt-0 mb-std p-std css-1ax1pfu css-errlgf'})\n",
    "    #table = soup.find_all('div', {'class':'row d-flex flex-wrap'})\n",
    "\n",
    "\n",
    "    \n",
    "    table = soup.find_all('h2', {'data-test':'employer-short-name'})\n",
    "    #job = soup.find_all('a',{'data-test':'cell-Jobs-url'})\n",
    "    job = soup.find_all('a',{'data-test':'cell-Jobs-url'})\n",
    "    l=[]\n",
    "    for link in soup.find_all('a',{'data-test':'cell-Jobs-url'}):\n",
    "     ##   print(link.get('href'))\n",
    "        l.append(link.get('href')) \n",
    "    employees = soup.find_all('span',{'data-test':'employer-size'})\n",
    "    #print(employees)\n",
    "    #print(job)\n",
    "    length=len(table)\n",
    "    #if (len(table) == 0):\n",
    "    #    break;\n",
    "\n",
    "    #print(table)\n",
    "    y = pd.DataFrame(l)\n",
    "    #print(y)\n",
    "    x = pd.DataFrame(table)\n",
    "    z = pd.DataFrame(employees)\n",
    "    df = df.append(x)\n",
    "    df2 = df2.append(y)\n",
    "    df3 = df3.append(z)\n",
    "    #print(len(df3))\n",
    "    #print(i)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up pulled data\n",
    "df.columns=['company']\n",
    "df2.columns=['job_url']\n",
    "df3.columns=['employees']\n",
    "df4 = pd.concat([df,df2,df3],axis=1, ignore_index=True)\n",
    "#df3 = pd.DataFrame(df,df2)\n",
    "df4.columns=['company','job_url','employees']\n",
    "df4=df4.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving checkpoint - save list of companies pulled\n",
    "df4.to_csv('top_worklife_companies_analyst_09162023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in saved file\n",
    "df4 = pd.read_csv('top_worklife_companies_analyst_09162023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc65991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38307eba",
   "metadata": {},
   "source": [
    "### Job Scrape\n",
    "\n",
    "Job Scrape from pulled companies From each of the companies scraped, the scraper will go to their company site and pull their current job openings. A call out here is that if the company has more than 40 job openings, only the first 40 jobs will be pulled as glassdoor limits the amount of jobs shown on the company profile page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54418291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "#df7 = pd.DataFrame()\n",
    "company_list = []\n",
    "j= 1\n",
    "length = 1\n",
    "for i in range(0,len(df4)):\n",
    "\n",
    "    x = df4['job_url'][i]\n",
    "    x = x[:-4]\n",
    "    x = x.split(\"-Jobs-\")\n",
    "\n",
    "    job_url='https://www.glassdoor.com' + x[0] + \"-Remote-Jobs-\" + x[1] + \"_P\" + str(j) + \".htm\"\n",
    "   # print(job_url)\n",
    "    company = df4['company'][i]\n",
    "  #  print(company)\n",
    "\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())#, chrome_options=options)\n",
    "    driver.get(job_url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    #print(html)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "   # print(soup)\n",
    "    job_title = soup.find_all('div', {'class':'job-title'})\n",
    "    job_location = soup.find_all('div',{'class':'css-k7ss21'})\n",
    "    #job = soup.find_all('a',{'data-test':'cell-Jobs-url'})\n",
    "    #job = soup.find_all('a',{'data-test':'cell-Jobs-url'})\n",
    "    l=[]\n",
    "    for link in soup.find_all('a',{'data-test':'job-link'}):\n",
    "    #    print(link.get('href'))\n",
    "        l.append(link.get('href')) \n",
    "        company_list.append(company)\n",
    "\n",
    "    #print(job)\n",
    "    #print(job_title)\n",
    "    length=len(job_title)\n",
    "   # print(length)\n",
    "    if (len(job_title) == 0):\n",
    "        j = 1\n",
    "    else:\n",
    "        j = j + 1\n",
    "\n",
    "    #print(table)\n",
    "    y = pd.DataFrame(l)\n",
    "    #print(y)\n",
    "    x = pd.DataFrame(job_title)\n",
    "    z = pd.DataFrame(job_location)\n",
    "    #employee_cnt = pd.DataFrame()\n",
    "    df5 = df5.append(x)\n",
    "    df6 = df6.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_list\n",
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up job dataframe pulled\n",
    "df5.columns=['job name']\n",
    "df6.columns=['job link']\n",
    "df7 = pd.concat([df5,df6],axis=1, ignore_index=True)\n",
    "#df3 = pd.DataFrame(df,df2)\n",
    "df7.columns=['job_name','job_link']\n",
    "df7=df7.reset_index()\n",
    "df7['company'] = company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb99db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df8 = pd.merge(df7, df4, how = 'inner', on = 'company')\n",
    "df8 = df8.drop(columns =['index_y','index_x','job_url'])\n",
    "#df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out jobs to only include certain keywords in job title\n",
    "df9 = df8[(df8['job_name'].str.contains('Analyst'))|\n",
    "         (df8['job_name'].str.contains('Data')) |\n",
    "         (df8['job_name'].str.contains('Scientist'))]\n",
    "\n",
    "df9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13794bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9= df9.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f57dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving checkpoint - save jobs df\n",
    "df9.to_csv('job_scrape_job_links_analyst_09162023_p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.read_csv('job_scrape_job_links_analyst_09162023_p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fa7d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df9.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be79f2",
   "metadata": {},
   "source": [
    "### Examine Job Description\n",
    "\n",
    "Examine jobs scraped and filter out those that don't fit your search criteria and see which of those are open to WFH policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a59969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df9 = pd.DataFrame()\n",
    "temp_l = []\n",
    "#might need to run this iteratively depending on the amount of jobs \n",
    "#you pulled and the memory on your pc. \n",
    "#Keeping it iteration to around 1000 jobs is recommended\n",
    "df10 = df9.head(1102)\n",
    "for i in range(0,len(df10)):\n",
    "    \n",
    "    print(i)\n",
    "    job_url2 = 'http://glassdoor.com' + df10['job_link'][i]\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(job_url2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    #job_desc = soup.find_all('div', {'class':'desc css-58vpdc ecgq1xb5'})\n",
    "    job_desc = soup.find_all('div', {'class':'desc css-c43yjn ecgq1xb5'})\n",
    "\n",
    "    #print(job_desc)\n",
    "    #if (job_desc.isempty()):\n",
    "    job_desc2 = soup.find_all('div', {'class':'css-15k9b95 elvm8840'})\n",
    "\n",
    "    #job_desc = soup.find_all('div', {'class':'css-15k9b95 elvm8840'})\n",
    "    #print(job_desc)\n",
    "    remote_check = str(job_desc).find('remote')\n",
    "    remote_check2 = str(job_desc).find('WFH')\n",
    "    remote_check3 = str(job_desc).find('home')\n",
    "    remote_check4 = str(job_desc2).find('remote')\n",
    "    remote_check5 = str(job_desc2).find('WFH')\n",
    "    remote_check6 = str(job_desc2).find('home')\n",
    "        \n",
    "    if ((remote_check > 0) | \n",
    "        (remote_check2 > 0 ) |\n",
    "        (remote_check3 > 0) |\n",
    "        (remote_check4 > 0 ) |\n",
    "        (remote_check5 > 0) |\n",
    "        (remote_check6 > 0) \n",
    "       ):\n",
    "        remote_final = 1\n",
    "    else:\n",
    "        remote_final = -1\n",
    "    #temp_df = pd.DataFrame(remote_final)\n",
    "\n",
    "    temp_l.append(remote_final)\n",
    "    #df9 = df9.append(temp_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#df9.columns = ['remote_check']\n",
    "df10['remote_check'] = temp_l#df9['remote_check']\n",
    "df10.to_csv('job_scrape_final_09122023_p6.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
